{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e8099f",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "1) They update the weights and bias<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11552e67",
   "metadata": {},
   "source": [
    "## Types of Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf2989",
   "metadata": {},
   "source": [
    "### 1) Gradient Descent\n",
    "\n",
    "<b>w_new = w_old - α *(dL/dw_old)</b>\n",
    "\n",
    "where α = learning rate<br>\n",
    "1) Gradient Descent uses the whole training data to update weight and bias. Suppose if we have millions of records then training becomes slow and computationally very expensive.\n",
    "\n",
    "### 2) Stochastic Gradient Descent\n",
    "\n",
    "<b>w_new = w_old -  α * (dL/dw_old)</b><br>\n",
    "\n",
    "1) Rather than going for batch processing, this optimizer focuses on performing one stochastic(random) update at a time. It is therefore usually much faster, also the cost function minimizes after each iteration (epoch)<br>\n",
    "2) SGD solved the Gradient Descent problem by using only single records to updates parameters. But, still, SGD is slow to converge because it needs forward and backward propagation for every record. And the path to reach global minima becomes very noisy.\n",
    "\n",
    "### 3) Mini Batch Gradient Descent\n",
    "\n",
    "1) Mini-batch GD overcomes the SGD drawbacks by using a batch of records to update the parameter. Since it doesn't use entire records to update parameter, the path to reach global minima is not as smooth as Gradient Descent.\n",
    "\n",
    "<img src=\"gd_sgd_mb_sgd.png\" hight=\"500\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a21e8",
   "metadata": {},
   "source": [
    "### 4) SGD with Momentum\n",
    "1) SGD with momentum leads to faster convergence<br>\n",
    "2) Exponentially Weighted Averages is used in sequential noisy data to reduce the noise and smoothen the data.<br>\n",
    "3) Thus present Gradient is dependent on its previous Gradient and so on. This accelerates SGD to converge faster and reduce the oscillation.\n",
    "\n",
    "<img src=\"sgd_with_momentum.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07715e8",
   "metadata": {},
   "source": [
    "### 5) Adagrad\n",
    "1) Unlike Stochastic Gradient descent, this optimizer uses a different learning rate for each iteration(epoch) rather than using the same learning rate for determining all the parameters.<br>\n",
    "2) Adagrad is well-suited for dealing with sparse data.<br>\n",
    "3) After a lot of iterations the alpha value becomes very large making the learning rate very small leading to no change between the new and the old weight. This in turn causes the learning rate to shrink and eventually become very small, where the algorithm is not able to acquire any further knowledge.<br>\n",
    "<img src=\"adagrad.png\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f845c9",
   "metadata": {},
   "source": [
    "### 6) Adadelta\n",
    "\n",
    "1) This is an extension of the Adaptive Gradient optimizer, taking care of its aggressive nature of reducing the learning rate infinitesimally.\n",
    "\n",
    "2) Here instead of using the previous squared gradients, the sum of gradients is defined as a reducing weighted average of all past squared gradients(weighted averages) this restricts the learning rate to reduce to a very small value.\n",
    "\n",
    "3) The idea behind Adadelta is that instead of summing up all the past squared gradients from 1 to “t” time steps, what if we could restrict the window size. For example, computing the squared gradient of the past 10 gradients and average out. This can be achieved using Exponentially Weighted Averages over Gradient.\n",
    "\n",
    "<img src=\"adagrad2.png\">\n",
    "\n",
    "The above equation shows that as the time steps “t” increase the summation of squared gradients “α” increases which led to a decrease in learning rate “η”. In order to resolve the exponential increase in the summation of squared gradients “α”, we replaced the “α” with exponentially weighted averages of squared gradients.\n",
    "\n",
    "<img src=\"adadelta.png\">\n",
    "\n",
    "Here unlike the alpha “α” in Adagrad, where it increases exponentially after every time step. In Adadelda, using the exponentially weighted averages over the past Gradient, an increase in “Sdw” is under control. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9469a5",
   "metadata": {},
   "source": [
    "### 7) Adam (Adaptive Moment Estimation)\n",
    "1) In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "2) Computationally efficient<br>\n",
    "3) Little memory requirements<br>\n",
    "4) Appropriate for problems with very noisy/or sparse gradients<br>\n",
    "<img src=\"adam1.png\" align=\"left\">\n",
    "<img src=\"adam2.png\" align=\"left\">\n",
    "<img src=\"adam3.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec8f69",
   "metadata": {},
   "source": [
    "<img src=\"optimizers_comparison1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db544f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sparse matrix\n",
    "\n",
    "# s = [\"welcome to python\",\"python is fun to learn\",\"learning python from scratch python\"]\n",
    "# total_unique_words = 9\n",
    "\n",
    "# uw = ['fun','from','is','learn','learning','python','scratch','to','welcome']\n",
    "\n",
    "# sm = [[0 0 0 0 0 1 0 1 1],\n",
    "#       [1 0 1 1 0 1 0 1 0],\n",
    "#       [0 1 0 0 1 2 1 0 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211bca3",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7716dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# Dense is fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765af99",
   "metadata": {},
   "source": [
    "### Model building steps using Sequential Layer\n",
    "1) Create a Sequential Layer<br>\n",
    "2) Add the required number Dense layers as Hidden layers and Output layer<br>\n",
    "3) Compile the model with appropriate optimizer and loss function<br>\n",
    "4) Fit the model on the training data<br>\n",
    "5) Generate prediction on the test data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a68b2",
   "metadata": {},
   "source": [
    "### Generalizations\n",
    "1) Activation Function used in Hidden Layer - relu,leaky_relu<br>\n",
    "2) Preferred Optimizer - adam<br>\n",
    "3) For Binary Classifier, activation function used in last layer - sigmoid<br>\n",
    "4) For Multiclass Classifier, activation function used in last layer - softmax<br>\n",
    "5) For Regression, activation function used in last layer - linear<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74de397",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5cb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=7,activation='relu',input_dim=4))     \n",
    "# H1 has 7 neurons, Input layer has 4 neurons\n",
    "model.add(Dense(units=10,activation='relu')) \n",
    "# H2 has 10 neurons\n",
    "model.add(Dense(units=12,activation='relu'))\n",
    "# H3 has 12 neurons\n",
    "model.add(Dense(units=1,activation='sigmoid'))  # [0,1]\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c0da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 7)                 35        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                80        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                132       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# H1 params = 4*7 + 7 = 35\n",
    "# H2 params = 7*10 + 10 = 80\n",
    "# H3 params = 12*10 + 12 = 132\n",
    "# Output layer params = 12*1 + 1 = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e030f",
   "metadata": {},
   "source": [
    "### Multiclass Classifier\n",
    "\n",
    "1) Use loss as categorical_crossentropy when target variable in LabelEncoded<br>\n",
    "2) Use loss as sparse_categorical_crossentropy when target variable in OneHotEncoded<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37cd1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mc = Sequential()\n",
    "model_mc.add(Dense(units=7,activation='relu',input_dim=4))     \n",
    "# H1 has 7 neurons, Input layer has 4 neurons\n",
    "model_mc.add(Dense(units=10,activation='relu')) \n",
    "# H2 has 10 neurons\n",
    "model_mc.add(Dense(units=12,activation='relu'))\n",
    "# H3 has 12 neurons\n",
    "model_mc.add(Dense(units=5,activation='softmax'))  # 5 categories in the target variable\n",
    "\n",
    "model_mc.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4edb7279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 7)                 35        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                80        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 12)                132       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 65        \n",
      "=================================================================\n",
      "Total params: 312\n",
      "Trainable params: 312\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_mc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabf335",
   "metadata": {},
   "source": [
    "### Types of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c2c908",
   "metadata": {},
   "source": [
    "<img src=\"Types_of_NN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weight assignment\n",
    "kernel-initializer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
