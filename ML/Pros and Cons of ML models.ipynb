{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f11c4c",
   "metadata": {},
   "source": [
    "### 1) Linear Regression\n",
    "\n",
    "#### Pros\n",
    "1) Linear regression performs well for linearly separable data<br>\n",
    "2) Easier to implement, interpret and efficient to train<br>\n",
    "\n",
    "#### Cons\n",
    "1) Prone to overfit<br>\n",
    "2) The assumption of linearity between dependent and independent variables may not hold good for each and very dataset<br>\n",
    "3) Linear regression is quite sensitive to outliers<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fdce0",
   "metadata": {},
   "source": [
    "### 2) Ridge Regression\n",
    "\n",
    "#### Pros\n",
    "1) Avoids overfitting a model.<br>\n",
    "2) The ridge estimator is preferably good at improving the least-squares estimate when there is multicollinearity.\n",
    "\n",
    "#### Cons\n",
    "1) They are unable to perform feature selection.<br>\n",
    "2) They trade the variance for bias.<br>\n",
    "3) Need to select optimal alpha (hyper parameter)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684263e",
   "metadata": {},
   "source": [
    "### 3) Lasso Regression\n",
    "\n",
    "#### Pros\n",
    "1) Select features, by shrinking co-efficient towards zero.<br>\n",
    "2) Avoids over fitting\n",
    "\n",
    "\n",
    "#### Cons\n",
    "1) Selected features will be highly biased.<br>\n",
    "2) For n<<p (n-number of data points, p-number of features), LASSO selects at most n features.<br>\n",
    "3) LASSO will select only one feature from a group of correlated features, the selection is arbitrary in nature.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb7c9c",
   "metadata": {},
   "source": [
    "### 3) Logistic Regression\n",
    "\n",
    "#### Pros\n",
    "1) Doesn’t assume linear relationship between independent and dependent variables.<br>\n",
    "2) Dependent variables does not need to be normally distributed.<br>\n",
    "3) No homogeneity of variance assumption required.<br>\n",
    "4) Effective interpretation of results.<br>\n",
    "\n",
    "#### Cons\n",
    "1) Effective mostly on linearly separable.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93571b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f839111b",
   "metadata": {},
   "source": [
    "### 4) KNN\n",
    "\n",
    "#### Pros\n",
    "1) No training period<br>\n",
    "2) Easy to implement<br>\n",
    "\n",
    "#### Cons\n",
    "1) Does not work well with high dimensions<br>\n",
    "2) Sensitive to noisy data, missing values and outliers<br>\n",
    "3) Does not work well with large data sets, as the cost of calculating distance is huge<br>\n",
    "4) Need feature scaling/standradization<br>\n",
    "5) Lazy algorithm<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc55810",
   "metadata": {},
   "source": [
    "### 5) Decision Tree\n",
    "\n",
    "#### Pros\n",
    "1) Does not require standardization and normalization.<br>\n",
    "2) Easy to implement<br>\n",
    "3) Less data preparation work<br>\n",
    "4) Provides feature importance estimate<br>\n",
    "\n",
    "#### Cons\n",
    "1) Doesn’t work for smooth boundaries<br>\n",
    "2) Due to greedy strategy, it has high variance which leads to overfitting<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0bc11",
   "metadata": {},
   "source": [
    "### 6) Random Forest\n",
    "\n",
    "#### Pros\n",
    "1) High performance and accurate<br>\n",
    "2) Provides feature importance estimate<br>\n",
    "3) No feature scaling is required<br>\n",
    "\n",
    "#### Cons\n",
    "1) Less interpret-ability, black box approach<br>\n",
    "2) Can over fit the data.<br>\n",
    "3) Requires more computational resources<br>\n",
    "4) Prediction time can be high<br>\n",
    "5) Tuning paramters can be time consuming<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0389be4",
   "metadata": {},
   "source": [
    "### 7) SVM\n",
    "\n",
    "#### Pros\n",
    "1) It works really well with a clear margin of separation<br>\n",
    "2) It is effective in high dimensional spaces.<br>\n",
    "3) It is effective in cases where the number of dimensions is greater than the number of samples.<br>\n",
    "4) It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "\n",
    "#### Cons\n",
    "1) It doesn’t perform well when we have large data set because the required training time is higher<br>\n",
    "2) Tuning parameters is also time consuming<br>\n",
    "3) SVMs perform poorly in imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185ce2b",
   "metadata": {},
   "source": [
    "### 8) Naive Bayes\n",
    "\n",
    "#### Pros\n",
    "1) Naive Bayes can work well with very small data sets.<br>\n",
    "2) It can even outperform highly advanced classification techniques.<br>\n",
    "3) It is quite simple, and can be build quickly. <br>\n",
    "4) Can be used to solve multi-class prediction problems as it’s quite useful with them.<br> \n",
    "5) Naive Bayes classifier performs better than other models with less training data if the assumption of independence of features holds. <br>\n",
    "\n",
    "\n",
    "#### Cons\n",
    "1) The assumption of independence doesnt hold good for all the datasets<br>\n",
    "2) It may suffer from Zero Frequency problem and Laplace smoothing technique is required to solve this issue<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12fff94",
   "metadata": {},
   "source": [
    "### 9) KMeans\n",
    "\n",
    "#### Pros\n",
    "1) Relatively simple to implement.<br>\n",
    "2) Scales to large data sets.<br>\n",
    "3) Guarantees convergence.<br>\n",
    "4) Can warm-start the positions of centroids.<br>\n",
    "5) Easily adapts to new examples.<br>\n",
    "\n",
    "\n",
    "#### Cons\n",
    "1) Sensitive to outliers<br>\n",
    "2) K-means has trouble clustering data where clusters are of varying sizes and density.\n",
    "2) Data needs to be scaled/standradized before applying KMeans<br>\n",
    "3) Number of clusters needs to be provided as a parameter<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71462ed3",
   "metadata": {},
   "source": [
    "### 10) Agglomerative Clustering\n",
    "\n",
    "#### Pros\n",
    "1) The agglomerative technique is easy to implement.<br>\n",
    "2) By the Agglomerative Clustering approach, smaller clusters will be created, which may discover similarities in data.\n",
    "\n",
    "\n",
    "#### Cons\n",
    "1) The usage of various distance metrics for measuring distances between the clusters may produce different results. So performing multiple experiments and then comparing the result is recommended to help the actual results’ veracity.<br>\n",
    "2) The agglomerative technique gives the best result in some cases only.<br>\n",
    "3) Dendrograms may become difficult to comprehend with larger datasets<br>\n",
    "4) High time complexity<br>\n",
    "5) The algorithm can never undo any previous steps. So for example, the algorithm clusters 2 points, and later on we see that the connection was not a good one, the program cannot undo that step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0defc47",
   "metadata": {},
   "source": [
    "### 11) PCA\n",
    "\n",
    "#### Pros\n",
    "1) Reduces overfitting<br>\n",
    "2) Removes correlated features<br>\n",
    "3) if the input dimensions are too high, then using PCA to speed up the algorithm is a reasonable choice to improves algorithms performance\n",
    "\n",
    "#### Cons\n",
    "1) Independent variables become less interpretable after PCA<br>\n",
    "2) Data standardization is must before PCA<br>\n",
    "3) Partial variance(information) loss depending of amount of final variance chosen to proceed with the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63831bad",
   "metadata": {},
   "source": [
    "### 12) AdaBoost\n",
    "\n",
    "#### Pros\n",
    "1) The accuracy of weak classifiers can be improved by using Adaboost<br>\n",
    "\n",
    "#### Cons\n",
    "1) Sensitive to Noisy data and outliers<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a147c3",
   "metadata": {},
   "source": [
    "### 13) XGBoost\n",
    "\n",
    "#### Pros\n",
    "1)  XGBoost is way cheaper than training neural networks because XGBoost does not need additional computation power which neural networks do (training XGBoost models does not require GPUs). XGboost and neural network models can both fit even very complex data sets.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1085e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
