{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c28efec9",
   "metadata": {},
   "source": [
    "### Equation of LinearRegression\n",
    "\n",
    "y = m1x1 + m2x2 + m2x3 + .... + mnxn \n",
    "\n",
    "\n",
    "#### Gradient Descent\n",
    "1) It is an optimization technique for finding minimum of a function.<br>\n",
    "2) It is first order iterative optimization<br>\n",
    "3) It consists of an Objective Function (also known as Cost Function or Error Function or Loss Function) that needs to be minimized.<br>\n",
    "4) Error function determines Error of the model on a given dataset<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d2c52",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "1) Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function.<br>\n",
    "\n",
    "2) This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression). \n",
    "\n",
    "3) Gradient descent algorithm does not work for all functions. There are two specific requirements.\n",
    "A function has to be:<br>\n",
    "<b>\n",
    "a) differentiable<br>\n",
    "b) convex<br>\n",
    "</b>    \n",
    "\n",
    "4) If a function is differentiable it has a derivative for each point in its domain\n",
    "Ex - <br>\n",
    "$ f(x) = {x^2}, f(x) = {x^3} - 5x $\n",
    "\n",
    "5) Second requirement is that function has to be convex. For a univariate function, this means that the line segment connecting two function’s points lays on or above its curve (it does not cross it). If it does it means that it has a local minimum which is not a global one.\n",
    "\n",
    "<img src=\"convex_non_convex.png\">\n",
    "\n",
    "Another way to check mathematically if a univariate function is convex is to calculate the second derivative and check if its value is always bigger than 0.\n",
    "\n",
    "$ f(x) = {x^2} - x + 3 $\n",
    "\n",
    "$ \\frac {df(x)}{dx}  = 2x - 1 $\n",
    "\n",
    "$ \\frac {d^2f(x)}{dx}  = 2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf26f8",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "Intuitively it is a slope of a curve at a given point in a specified direction.\n",
    "\n",
    "### Gradient Descent\n",
    "Imagine a valley and a person with no sense of direction who wants to get to the bottom of the valley. He goes down the slope and takes large steps when the slope is steep and small steps when the slope is less steep. He decides his next position based on his current position and stops when he gets to the bottom of the valley which was his goal.\n",
    "\n",
    "<img src=\"grad_desc1.png\" height=\"300\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b042d000",
   "metadata": {},
   "source": [
    "### Steps to perform Gradient Descent\n",
    "\n",
    "1) Compute Loss Function<br>\n",
    "a) Find the difference between the actual y and predicted y value(y = mx + c), for a given x.<br>\n",
    "b) Square this difference.<br>\n",
    "c) Find the mean of the squares for every value in X.<br>\n",
    "\n",
    "<img src=\"cost_fun.png\" height=\"150\" width=\"250\">\n",
    "<img src=\"cost_fun2.png\" height=\"150\" width=\"250\">\n",
    "\n",
    "<b>over here $ \\bar{y}_{i} $ is the ypred and not y_mean </b>\n",
    "\n",
    "2) Initially let m = 0 and c = 0. Let L be our learning rate. This controls how much the value of m changes with each step. L could be a small value like 0.0001 for good accuracy.<br>\n",
    "\n",
    "3) Calculate the partial derivative of the loss function with respect to m, and plug in the current values of x, y, m and c in it to obtain the derivative value D.\n",
    "\n",
    "where $ {D}_{m} $ is the value of the partial derivative with respect to m\n",
    "and $ {D}_{c} $ is the value of the partial derivative with respect to c\n",
    "\n",
    "<img src=\"cost_fun_der1.png\" height=\"250\" width=\"350\">\n",
    "<img src=\"cost_fun_der2.png\" height=\"150\" width=\"250\">\n",
    "\n",
    "<br>\n",
    "4) Now we update the current value of m and c using the following equation:\n",
    "<img src=\"cost_fun_m_c_update.png\" height=\"150\" width=\"250\">\n",
    "\n",
    "5) We repeat this process until our loss function is a very small value or ideally 0 (which means 0 error or 100% accuracy). The value of m and c that we are left with now will be the optimum values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1c142",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "There’s an important parameter L which scales the gradient and thus controls the step size. \n",
    "In machine learning, it is called learning rate and have a strong influence on performance.<br>\n",
    "\n",
    "a) The smaller learning rate the longer GD converges, or may reach maximum iteration before reaching the optimum point<br>\n",
    "b) If learning rate is too big the algorithm may not converge to the optimal point (jump around) or even to diverge completely.<br>\n",
    "\n",
    "<img src=\"lr_impact.png\" height=\"300\" width=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e59282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2cb5780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.502345</td>\n",
       "      <td>31.707006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.426804</td>\n",
       "      <td>68.777596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.530358</td>\n",
       "      <td>62.562382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.475640</td>\n",
       "      <td>71.546632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.813208</td>\n",
       "      <td>87.230925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x          y\n",
       "0  32.502345  31.707006\n",
       "1  53.426804  68.777596\n",
       "2  61.530358  62.562382\n",
       "3  47.475640  71.546632\n",
       "4  59.813208  87.230925"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gd_data.csv')\n",
    "df.columns = ['x','y']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df02645",
   "metadata": {},
   "source": [
    "#### Q) Create a function to compute MSE (Cost Function) for y based on x using different values of m. \n",
    "Consider ypred = m*x<br>\n",
    "Consider different m = [0,0.5,1,1.5,2,2.5,3]\n",
    "\n",
    "Plot the coefficients vs mse(cost function) on a scatter chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277467a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c242f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e52c15",
   "metadata": {},
   "source": [
    "### Performance of Gradient Descent\n",
    "\n",
    "<img src=\"performance_of_grad_desc.png\" height=\"300\" width=\"450\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1133edc",
   "metadata": {},
   "source": [
    "### SGD (Stochastic Gradient Descent)\n",
    "1) In SGD, it uses only single sample to perform each iteration.<br>\n",
    "2) Sample is randomly shuffled and selected for performing iteration<br>\n",
    "3) SGD reaches convergence much faster than GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212182f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152d667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129831e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f702cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3610c251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
