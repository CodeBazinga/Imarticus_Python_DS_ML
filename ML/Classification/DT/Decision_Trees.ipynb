{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link Used\n",
    "1) https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "1) Decision-tree algorithm falls under the category of supervised learning algorithms. <br>\n",
    "2) Decision Tress is used for both classification and regression. <br>\n",
    "3) <b>A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).</b><br>\n",
    "4) The decision tree from the name itself signifies that it is used for making decisions from the given dataset. The concept behind the decision tree is that it helps to select appropriate features for splitting the tree into subparts.<br>\n",
    "5) It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.<br>\n",
    "6) Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label(target variable) and attributes are represented on the internal node of the tree.<br>\n",
    "\n",
    "Example<br>\n",
    "We have 100 rows of data with<br>\n",
    "x = 'Age', 'Gender', 'BMI','Body_wt', 'Blood_Glucose_Level'<br>\n",
    "y = 0 or 1 [0 = Non-diabteic, 1 = Diabetic]<br>\n",
    "65 are diabetic and 35 are non-diabetic\n",
    "\n",
    "<img src=\"dt1.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<img src=\"DT_example.png\" height=\"300\" width=\"450\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm\n",
    "1) We use statistical methods <b>(gini or entropy)</b> for ordering attributes as root or the internal node.\n",
    "\n",
    "### 1) Entropy\n",
    "Sample Reference -> https://www.saedsayad.com/decision_tree.htm\n",
    "\n",
    "1)\tEntropy is the measure of uncertainty or disorder or impurity .It characterizes the impurity of an arbitrary collection of examples. The higher the entropy more the information content.<br>\n",
    "2)\tIf the sample is completely homogeneous the entropy is zero and if the sample is equally divided it has entropy of one.\n",
    "<img src=\"dt2.png\">\n",
    "where<br>\n",
    "pi = probability of label i<br>\n",
    "n = number of categories in the label <br>\n",
    "\n",
    "<b>Information Gain</b><br>\n",
    "a)\tIt is based on decrease in entropy after dataset is split on an attribute/(feature or column)<br>\n",
    "b)\tThe split with highest information gain is chosen as an internal node.<br>\n",
    "\n",
    "<b>\n",
    "E(Target_variable) = Σ-plog(p)  <br>\n",
    "E(Target_variable,feature) = P(feature)*E(Target) <br>\n",
    "Information Gain = Entropy(Target_variable) - Entropy(Target_variable,feature), \n",
    "</b>\n",
    "    \n",
    "### 2) Gini\n",
    "\n",
    "1)\tGini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.<br>\n",
    "2)\tIt means an attribute with lower Gini index should be preferred.<br>\n",
    "3)\t<b>Gini = 1 – Σ (Pi)^2 for i=1 to number of classes</b><br>\n",
    "4) Gini impurity has a maximum value of 0.5, which is the worst we can get, and a minimum value of 0 means the best we can get.\n",
    "\n",
    "Example -> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "1) criterion - entropy or gini\n",
    "\n",
    "2) max_depth - max depth of decision tree\n",
    "\n",
    "3) min_samples_split - Min number of samples beyond which an internal node gets coverted to a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Link Used\n",
    "1) https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/<br>\n",
    "    \n",
    "2) CHAID -> https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/<br>\n",
    "    \n",
    "3) C4.5 ->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithms\n",
    "1) <b>ID3 </b><br>\n",
    "Iterative Dichotomiser<br>\n",
    "ID3 decision tree algorithm uses Information Gain to decide the splitting points. In order to measure how much information we gain, we can use entropy to calculate the homogeneity of a sample.\n",
    "\n",
    "2) <b>C4.5</b><br>\n",
    "Can create a more generalized models including continuous data and could handle missing data. Additionally, some resources such as Weka named this algorithm as J48. Actually, it refers to re-implementation of C4.5 release 8.\n",
    "\n",
    "\n",
    "3) <b>CART</b><br>\n",
    "CART (Classification and Regression Tree)<br>\n",
    "CART uses the Gini method to create split points including Gini Index (Gini Impurity) and Gini Gain.\n",
    "\n",
    "\n",
    "Gini - \n",
    "a) The probability of assigning a wrong label to a sample by picking the label randomly and is also used to measure feature importance in a tree.<br>\n",
    "b) calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly. \n",
    "\n",
    "4) <b>CHAID</b><br>\n",
    "Chi-square automatic interaction detection<br>\n",
    "The higher the value, the higher the statistical significance. Similar to the others, CHAID builds decision trees for classification problems. This means that it expects data sets having a categorical target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Decision Trees_Ex1-MWF-10.-11.30am.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
