{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ebf2989",
   "metadata": {},
   "source": [
    "### 1) Gradient Descent\n",
    "\n",
    "<b>w_new = w_old - α *(dL/dw_old)</b>\n",
    "\n",
    "1) Gradient Descent uses the whole training data to update weight and bias. Suppose if we have millions of records then training becomes slow and computationally very expensive.\n",
    "\n",
    "### 2) Stochastic Gradient Descent\n",
    "\n",
    "<b>w_new = w_old -  α * (dL/dw_old)</b><br>\n",
    "\n",
    "1) Rather than going for batch processing, this optimizer focuses on performing one stochastic update at a time. It is therefore usually much faster, also the cost function minimizes after each iteration (epoch)<br>\n",
    "2) SGD solved the Gradient Descent problem by using only single records to updates parameters. But, still, SGD is slow to converge because it needs forward and backward propagation for every record. And the path to reach global minima becomes very noisy.\n",
    "\n",
    "### 3) Mini Batch Gradient Descent\n",
    "\n",
    "1) Mini-batch GD overcomes the SDG drawbacks by using a batch of records to update the parameter. Since it doesn't use entire records to update parameter, the path to reach global minima is not as smooth as Gradient Descent.\n",
    "\n",
    "### 4) SGD with Momentum\n",
    "1) SGD with momentum leads to faster convergence<br>\n",
    "2) Exponentially Weighted Averages is used in sequential noisy data to reduce the noise and smoothen the data.<br>\n",
    "3) Thus present Gradient is dependent on its previous Gradient and so on. This accelerates SGD to converge faster and reduce the oscillation.\n",
    "\n",
    "<img src=\"sgd_with_momentum.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07715e8",
   "metadata": {},
   "source": [
    "### 5) Adagrad\n",
    "1) Unlike Stochastic Gradient descent, this optimizer uses a different learning rate for each iteration(epoch) rather than using the same learning rate for determining all the parameters.<br>\n",
    "2) Adagrad is well-suited for dealing with sparse data.<br>\n",
    "3) After a lot of iterations the alpha value becomes very large making the learning rate very small leading to no change between the new and the old weight. This in turn causes the learning rate to shrink and eventually become very small, where the algorithm is not able to acquire any further knowledge.<br>\n",
    "<img src=\"adagrad.png\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f845c9",
   "metadata": {},
   "source": [
    "### 6) Adadelta\n",
    "\n",
    "1) This is an extension of the Adaptive Gradient optimizer, taking care of its aggressive nature of reducing the learning rate infinitesimally.\n",
    "\n",
    "2) Here instead of using the previous squared gradients, the sum of gradients is defined as a reducing weighted average of all past squared gradients(weighted averages) this restricts the learning rate to reduce to a very small value.\n",
    "\n",
    "3) The idea behind Adadelta is that instead of summing up all the past squared gradients from 1 to “t” time steps, what if we could restrict the window size. For example, computing the squared gradient of the past 10 gradients and average out. This can be achieved using Exponentially Weighted Averages over Gradient.\n",
    "\n",
    "<img src=\"adagrad2.png\">\n",
    "\n",
    "The above equation shows that as the time steps “t” increase the summation of squared gradients “α” increases which led to a decrease in learning rate “η”. In order to resolve the exponential increase in the summation of squared gradients “α”, we replaced the “α” with exponentially weighted averages of squared gradients.\n",
    "\n",
    "<img src=\"adadelta.png\">\n",
    "\n",
    "Here unlike the alpha “α” in Adagrad, where it increases exponentially after every time step. In Adadelda, using the exponentially weighted averages over the past Gradient, an increase in “Sdw” is under control. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9469a5",
   "metadata": {},
   "source": [
    "### 7) Adam (Adaptive Moment Estimation)\n",
    "1) In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "2) Computationally efficient<br>\n",
    "3) Little memory requirements<br>\n",
    "4) Appropriate for problems with very noisy/or sparse gradients<br>\n",
    "<img src=\"adam1.png\" align=\"left\">\n",
    "<img src=\"adam2.png\" align=\"left\">\n",
    "<img src=\"adam3.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb99e876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd1aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
