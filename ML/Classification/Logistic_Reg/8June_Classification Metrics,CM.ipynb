{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2265d893",
   "metadata": {},
   "source": [
    "### Classification Problems\n",
    "1) Dependent variable is categorical. Examples - Yes or No, 0 or 1, True or  False<br>\n",
    "2) Classification can be binary classfication (2 categories) or multiclass classfication(more than 2 categories)<br>\n",
    "Example<br>\n",
    "x = Age, Gender, BMI, Blood Glucose Level, Body_Weight<br>\n",
    "y = Diabetic or Non-diabetic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247dd39",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "1) It is a square matrix consisting of 4 terms (TP,FN,FP,TN)<br>\n",
    "2) It is the basis of classification metrics.<br>\n",
    "3) If the number of categories is n, shape of confusion matrix will be (n,n)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8471c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# 0 - Positive(True), 1 - Negative(False)\n",
    "y_true = [0,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,1,0,1,1,1,1]\n",
    "y_pred = [1,0,1,0,1,1,0,0,0,1,1,0,1,0,1,0,0,1,1,1,0,1]\n",
    "print(len(y_true))\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38738439",
   "metadata": {},
   "source": [
    "### Terminologies\n",
    "1) <b>TP(True Positive): </b><br>\n",
    "Actual value is positive, ML model also predicted a positive value\n",
    "\n",
    "2) <b>FN(False Negative): </b><br>\n",
    "Actual value is positive, ML model predicted a negative value\n",
    "\n",
    "3) <b>FP(False Positive): </b><br>\n",
    "Actual value is negative, ML model predicted a positive value\n",
    "\n",
    "4) <b>TN(True Negative): </b><br>\n",
    "Actual value is negative, ML model also predicted a negative value\n",
    "\n",
    "\n",
    "### Note\n",
    "1) Sum of all the cases where actual value is Positive = TP + FN <br> \n",
    "2) Sum of all the cases where actual value is Negative = TN + FP<br>\n",
    "3) Sum of all the cases where predicted value is Positive = TP + FP<br>\n",
    "4) Sum of all the cases where predicted value is Negative = TN + FN<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5d760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93929e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - Positive(True), 1 - Negative(False)\n",
    "y_true = [0,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,1,0,1,1,1,1]\n",
    "y_pred = [1,0,1,0,1,1,0,0,0,1,1,0,1,0,1,0,0,1,1,1,0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b188e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true,y_pred)\n",
    "# print(cm)\n",
    "# Actual value=0,Predicted value = 0 =>\n",
    "# Actual value=0,Predicted value = 1 =>\n",
    "# Actual value=1,Predicted value = 0 =>\n",
    "# Actual value=1,Predicted value = 1 =>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22af2d",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "1) <b> Precision - TP/(TP+FP) , TN/(TN+FN)</b><br>\n",
    "From all the cases where predicted value is Positive, how many are actually positive.<br>\n",
    "From all the cases where predicted value is Negative, how many are actually negative\n",
    "\n",
    "2) <b>Recall - TP/(TP+FN), TN/(TN+FP)</b><br>\n",
    "From all cases where actual value is Positive, how many the model has predicted Positive.<br>\n",
    "From all cases where actual value is Negative, how many the model has predicted Negative.\n",
    "\n",
    "Recall TP/(TP+FN) is also called <b>Sensitivity or True Positive Rate (TPR)</b>\n",
    "\n",
    "3) <b>F1-Score - 2 * Precision * Recall/(Precision + Recall)</b><br>\n",
    "It is harmonic mean between Precision and Recall<br>\n",
    "\n",
    "4) <b>Accuracy - (TP + TN)/(TP + FN + FP + TN)</b><br>\n",
    "Overall how good the prediction are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f4fbd",
   "metadata": {},
   "source": [
    "### Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432a287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93def5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb53c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b7f2f2",
   "metadata": {},
   "source": [
    "### ROC AUC-Curve\n",
    "\n",
    "1) ROC(Receiver Operator Characteristics) is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
    "\n",
    "2) The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n",
    "\n",
    "3) When AUC = 1, then the classifier is able to perfectly distinguish between all the Positive and the Negative class points correctly. If, however, the AUC had been 0, then the classifier would be predicting all Negatives as Positives, and all Positives as Negatives.\n",
    "\n",
    "4) When 0.5<AUC<1, there is a high chance that the classifier will be able to distinguish the positive class values from the negative class values. This is so because the classifier is able to detect more numbers of True positives and True negatives than False negatives and False positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b2425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01b5d8a2",
   "metadata": {},
   "source": [
    "### Example\n",
    "On y-axis, we have probability of obese and on x-axis, we have weight<br>\n",
    "1 - Obese(Positive), 0 - Not Obese (Negative)\n",
    "\n",
    "<img src=\"roc_auc1.png\" height=\"250\" width=\"350\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53f441",
   "metadata": {},
   "source": [
    "### Threshold = 0.5\n",
    "<img src=\"roc_auc3.png\" height=\"250\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90428799",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write tehe confusion matrix for the above threshold\n",
    "# 1 - Obese(Positve), 0 - Not Obese (Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e82f70",
   "metadata": {},
   "source": [
    "### Threshold <0\n",
    "<img src=\"roc_auc6.png\" height=\"250\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d20636",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write tehe confusion matrix for the above threshold\n",
    "# 1 - Obese(Positve), 0 - Not Obese (Negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c945fa",
   "metadata": {},
   "source": [
    "### Threshold=0.1\n",
    "<img src=\"roc_auc7.png\" height=\"250\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547bb3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write tehe confusion matrix for the above threshold\n",
    "# 1 - Obese(Positve), 0 - Not Obese (Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192514c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bef353ab",
   "metadata": {},
   "source": [
    "#### ROC graphs \n",
    "1) ROC stands for Receiver Operator Characteristis graph<br>\n",
    "2) ROC graphs provide a simple way to summarizes all theconfusion matrix that each threshold produced<br>\n",
    "3) ROC graph is made by considering FPR(False positive rate on x-axis) and TPR(True Positive Rate on y-axis).<br>\n",
    "4) TPR = Sensitivity = TP/(TP+FN)<br>\n",
    "5) FPR = (1-Specificity) = FP/(FP + TN)\n",
    "\n",
    "where <br>\n",
    "TP + FN = Sum of all the cases where actual value is Positive<br>\n",
    "FP + TN = Sum of all the cases where actual value is Negative<br>\n",
    "\n",
    "\n",
    "#### AUC Curve\n",
    "1) AUC stands for Area under curve.<br>\n",
    "2) It makes it easire to compare one ROC graph with another.<br>\n",
    "<img src=\"auc_curve1.png\" height=\"250\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976e27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0028d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = [1,0.75,0.5,0.25,0,0,0,0]\n",
    "tpr = [1,1,1,0.75,0.75,0.5,0.25,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a56f1b",
   "metadata": {},
   "source": [
    "#### Q)\n",
    "1) Plot a scatter chart between fpr (on x-axis) and tpr on (y-axis)<br>\n",
    "2) Plot a line chart for TPR=FPR<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d6f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
