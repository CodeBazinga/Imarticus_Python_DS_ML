{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "847a7634",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "1) Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret data through a kind of machine perception and process input data to solve a certain problem. The patterns they recognize are numerical, contained in vectors, into which all real-world data i.e. text, images, sound can fit in.\n",
    "\n",
    "2) Neural networks are a class of machine learning algorithms used to model complex patterns in datasets using multiple hidden layers and activation functions. A neural network takes an input, passes it through multiple layers of hidden neurons (mini-functions with unique coefficients that must be learned), and outputs a prediction representing the combined input of all the neurons.\n",
    "\n",
    "\n",
    "3) Neural networks are trained iteratively using optimization techniques like gradient descent. After each cycle of training, an error metric is calculated based on the difference between prediction and target. The derivatives of this error metric are calculated and propagated back through the network using a technique called <b>backpropagation</b>. Each neuron’s coefficients (weights) are then adjusted relative to how much they contributed to the total error. This process is repeated iteratively until the network error drops below an acceptable threshold.\n",
    "\n",
    "4) In a neural network, each neuron (except those in the input layer) is actually a sum of all its inputs; which are in fact the outputs from the previous layer multiplied by some weights. An additional term called bias is added to this sum. And a nonlinear function known as activation function is applied to the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a02e3",
   "metadata": {},
   "source": [
    "### Layers in Neural network\n",
    "1) <b>Input Layer </b><br>\n",
    "Holds the data your model will train on. Each neuron in the input layer represents a unique attribute in your dataset (e.g. height, hair color, etc.).\n",
    "\n",
    "2) <b>Hidden Layer </b><br>\n",
    "Sits between the input and output layers and applies an activation function before passing on the results. There are often multiple hidden layers in a network. In general, hidden layers are typically fully-connected layers — each neuron receives input from all the previous layer’s neurons and sends its output to every neuron in the next layer.\n",
    "\n",
    "3) <b>Output Layer</b><br> \n",
    "The final layer in a network. It receives input from the previous hidden layer, optionally applies an activation function, and returns an output representing your model’s prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08665096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4582fe08",
   "metadata": {},
   "source": [
    "### Terminologies\n",
    "<b>Neuron</b><br>\n",
    "A neuron takes a group of weighted inputs, applies an activation function, and returns an output. Inputs to a neuron can either be features from a training set or outputs from a previous layer’s neurons.\n",
    "\n",
    "<b>Weight</b><br>\n",
    "Weights are applied to the inputs as they travel along synapses to reach the neuron and a bias  term is added. The neuron then applies an activation function to the “sum of weighted inputs added to the bias” from each incoming synapse and passes the result on to all the neurons in the next layer.<br>\n",
    "\n",
    "<b>Bias</b><br>\n",
    "Bias helps in controlling the value at which  activation  function will trigger.  Bias allows you to shift the activation function by adding a constant<br>\n",
    "\n",
    "\n",
    "<b>Activation Function</b><br>\n",
    "1) An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.<br>\n",
    "2) An activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data.<br>\n",
    "3) Activation function also helps to normalize the output of any input in the range between 1 to -1 or 0 to 1. <br>\n",
    "\n",
    "<b>Synapse</b><br>\n",
    "Synapses are like roads in a neural network. They connect inputs to neurons, neurons to neurons, and neurons to outputs.\n",
    "\n",
    "<b>Optimizer</b><br>\n",
    "1) It is very important to tweak the weights of the model during the training process, to make our predictions as correct and optimized as possible. <br>\n",
    "2) This is where optimizers come into picture. They tie together the loss function and model parameters by updating the model in response to the output of the loss function.<br>\n",
    "\n",
    "<b>Loss Function/Cost Function</b><br>\n",
    "1) It is a parameter in model’s predict function that tells us “how good” the model is at making predictions for a given set of parameters. <br>\n",
    "2) The loss function has its own curve and its own derivatives. The slope of this curve tells us how to change our parameters to make the model more accurate. We use the cost function to update our parameters.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05df41",
   "metadata": {},
   "source": [
    "#### General Equation of Neural Network\n",
    "y = σ((sum(wi * xi)) + b)\n",
    "\n",
    "where<br>\n",
    "y = output<br>\n",
    "xi = input<br>\n",
    "wi = weight<br>\n",
    "b = bias<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0699e",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "1) An activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data.<br>\n",
    "2) When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.<br>\n",
    "3) It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell.<br>\n",
    "4) It normalizes the [sum(input * weight) + bias] in a specified range depending upon the activation function used.  This is important because input into the activation function is W * x + b where W is the weights of the cell and the x is the inputs and then there is the bias b added to that. This value if not restricted to a certain limit can go very high in magnitude especially in case of very deep neural networks that have millions of parameters.<br>\n",
    "Ex - sigmoid transforms its input in the range [0,1]<br>\n",
    "\n",
    "5) The most important feature in an activation function is its ability to add non-linearity into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb04ad4",
   "metadata": {},
   "source": [
    "### Desirable Features of Activation Function\n",
    "1) <b>Continuously Differentiable:</b><br> \n",
    "Neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer.\n",
    "\n",
    "2) <b>Zero-Centered:</b><br> Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction.\n",
    "\n",
    "3) <b>Monotonic</b><br> \n",
    "A function which is either entirely non-increasing or non-decreasing is monotnic. The monotonicity criterion helps the neural network to converge easier into an more accurate classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6df4ef",
   "metadata": {},
   "source": [
    "### Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cca1f",
   "metadata": {},
   "source": [
    "### 1) Linear\n",
    "Range: -infinity to +infinity<br>\n",
    "Function: f(x) = a.x<br>\n",
    "Derivative is a constant<br>\n",
    "Used in ANN Regression in the last layer<br>\n",
    "<img src=\"linear_af.png\" height=\"350\" width=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee0ce3",
   "metadata": {},
   "source": [
    "### 2) Sigmoid\n",
    "Range: 0 to 1<br>\n",
    "Suffers from Vanishing Gradient problem<br>\n",
    "It is non-linear,differentiable, monotonic but not zero-centered<br>\n",
    "Used in ANN Binary Classification in the last layer<br>\n",
    "<img src=\"sigmoid_af.png\" height=\"350\" width=\"350\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200d1b5",
   "metadata": {},
   "source": [
    "### 3) Tanh \n",
    "Range: -1 to 1\n",
    "Suffers from Vanishing Gradient problem<br>\n",
    "It is non-linear,differentiable, monotonic and zero-centered<br>\n",
    "Used in LSTM<br>\n",
    "<img src=\"tanh_af.png\" height=\"350\" width=\"350\" align=\"left\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a4847",
   "metadata": {},
   "source": [
    "### 4) RELU (Rectified Linear Unit)\n",
    "Range: 0 to +infinity<br>\n",
    "Function: <b>f(x) = max(0,x)</b><br>\n",
    "Used in hidden layers<br>\n",
    "Suffers from Dying relu problem - Since the output is zero for all negative inputs. It causes some nodes to completely die and not learn anything.<br>\n",
    "<img src=\"relu_af.png\" height=\"250\" width=\"250\" align=\"left\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc583a6",
   "metadata": {},
   "source": [
    "### 5) Leaky ReLU and Parametric ReLU\n",
    "Function: max(αx, x)<br>\n",
    "Leaky ReLU solves the Dying ReLU problem to some extent.<br>\n",
    "Can be used in hidden layers as a replacement for RELU<br>\n",
    "<img src=\"leaky_relu_af.png\" height=\"250\" width=\"250\" align=\"left\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed43c65",
   "metadata": {},
   "source": [
    "### 6) Softmax\n",
    "1) The softmax function is sometimes called the soft argmax function, or multi-class logistic regression<br>\n",
    "2) This is because the softmax is a generalization of logistic regression that can be used for multi-class classification, and its formula is very similar to the sigmoid function which is used for logistic regression. <br>\n",
    "3) Converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector.<br>\n",
    "4) It is used in multiclass classification in ANN and CNN.\n",
    "<img src=\"softmax_af.png\" height=\"270\" width=\"270\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45c71d",
   "metadata": {},
   "source": [
    "### 7) ELU (Exponential Linear Unit)\n",
    "<img src=\"elu_af.png\" height=\"200\" width=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b6580",
   "metadata": {},
   "source": [
    "### Problems faced by Activation Functions<br>\n",
    "\n",
    "#### Vanishing Gradient\n",
    "1) As the backpropagation algorithm advances backward from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged. As a result, the gradient descent never converges to the optimum. This is known as the vanishing gradients problem.\n",
    "\n",
    "#### Exploding Gradient\n",
    "1) On the contrary, in some cases, the gradients keep on getting larger and larger as the backpropagation algorithm progresses. This, in turn, causes very large weight updates and causes the gradient descent to diverge. This is known as the exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4edb2",
   "metadata": {},
   "source": [
    "### All about activation Functions\n",
    "https://mlfromscratch.com/activation-functions-explained/#/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5815cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
